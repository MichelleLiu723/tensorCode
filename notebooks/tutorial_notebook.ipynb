{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Tutorial for the Revised Tensor Network Code\n",
    "Here's a quick overview of how to use the revised code for optimizing a tensor network (TN) with fixed ranks _(credit: Michelle for writing the initial version of the code)_. The current version is a bit more modular, so that we can use the same continuous optimization routine for each of the problems being solved. The ingredients that still need to be implemented are:\n",
    "\n",
    "1. Code implementing the different discrete optimization procedures\n",
    "2. Loss function for the tensor completion task, which takes in our TN and dataset of known tensor elements, and returns a loss based on the average loss in each of these known elements\n",
    "\n",
    "Before starting those pieces though, it would be helpful to first understand how to interface with the continuous optimization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import torch\n",
    "import tensornetwork as tn\n",
    "\n",
    "# Make sure notebook can find the core code\n",
    "sys.path.append(\"..\")\n",
    "import core_code as cc\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "_ = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing TNs\n",
    "Tensor networks of arbitrary rank are initialized with the `random_tn` function, with the rank being specified in one of several ways. Scalar values of the `rank` argument give constant-rank TNs (default: rank=1), but individual ranks are also possible using a list as input. Note how the ranks are being displayed, with upper diagonal entries giving the TN ranks, and diagonal entries giving the input dimension of each core _(credit: Meraj for the idea of putting the input dims on the diagonals)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-1 TN has ranks\n",
      "tensor([[2, 1, 1, 1],\n",
      "        [0, 4, 1, 1],\n",
      "        [0, 0, 5, 1],\n",
      "        [0, 0, 0, 6]])\n",
      "...and input dimensions\n",
      "(2, 4, 5, 6)\n",
      "\n",
      "Shape of TN is torch.Size([2, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "# The dimensions of each of the inputs to our tensor network (TN)\n",
    "input_dims = [2, 4, 5, 6]\n",
    "\n",
    "# Initialize a random rank-1 TN (default rank is 1)\n",
    "example_tn = cc.random_tn(input_dims)\n",
    "\n",
    "print(\"Rank-1 TN has ranks\")\n",
    "cc.print_ranks(example_tn)\n",
    "print(\"...and input dimensions\")\n",
    "print(cc.get_indims(example_tn))\n",
    "\n",
    "# TNs can be expanded into dense tensors with expand_network\n",
    "print(f\"\\nShape of TN is {cc.expand_network(example_tn).shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-10 TN has ranks\n",
      "tensor([[ 2, 10, 10, 10],\n",
      "        [ 0,  4, 10, 10],\n",
      "        [ 0,  0,  5, 10],\n",
      "        [ 0,  0,  0,  6]])\n",
      "...and has 17000 parameters\n",
      "\n",
      "Irregularly-shaped goal TN has ranks\n",
      "tensor([[ 2,  1,  2,  3],\n",
      "        [ 0,  4,  5,  8],\n",
      "        [ 0,  0,  5, 13],\n",
      "        [ 0,  0,  0,  6]])\n",
      "This TN has 2694 parameters\n"
     ]
    }
   ],
   "source": [
    "# Random TNs with higher ranks can also be defined\n",
    "big_tn = cc.random_tn(input_dims, rank=10)\n",
    "\n",
    "print(\"Rank-10 TN has ranks\")\n",
    "cc.print_ranks(big_tn)\n",
    "print(f\"...and has {cc.num_params(big_tn)} parameters\")\n",
    "print()\n",
    "\n",
    "# As our trainable model, let's use a rank-3 TN\n",
    "base_tn = cc.random_tn(input_dims, rank=3)\n",
    "\n",
    "# Individual ranks of TN edges can be set with upper-triangular format\n",
    "# (this is the TN used as a target in the following)\n",
    "rank_list = [[1,2,3], \n",
    "               [5,8], \n",
    "                [13]]\n",
    "goal_tn = cc.random_tn(input_dims, rank=rank_list)\n",
    "\n",
    "print(\"Irregularly-shaped goal TN has ranks\")\n",
    "cc.print_ranks(goal_tn)\n",
    "print(f\"This TN has {cc.num_params(goal_tn)} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor Recovery\n",
    "\n",
    "Let's now see how we use continuous optimization in a tensor recovery problem. The key difference with the earlier version of the code is that a loss function is input to `continuous_optim`, which specifies the type of problem being solved.\n",
    "\n",
    "In this case, we use `tensor_recovery_loss`, but generally the loss function must take in the TN being trained and a problem-dependent data format, and return a loss. In other words:\n",
    "\n",
    "`loss_value = loss_fun(our_tn, target_data)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EPOCH 1 \n",
      "    Train loss: 10856.338\n",
      "  EPOCH 2 \n",
      "    Train loss: 10659.803\n",
      "  EPOCH 3 \n",
      "    Train loss: 10486.983\n",
      "  EPOCH 4 \n",
      "    Train loss: 10333.783\n",
      "  EPOCH 5 \n",
      "    Train loss: 10196.966\n",
      "  EPOCH 6 \n",
      "    Train loss: 10073.937\n",
      "  EPOCH 7 \n",
      "    Train loss: 9962.594\n",
      "  EPOCH 8 \n",
      "    Train loss: 9861.219\n",
      "  EPOCH 9 \n",
      "    Train loss: 9768.391\n",
      "  EPOCH 10 \n",
      "    Train loss: 9682.927\n",
      "\n",
      "Train loss went from 10856.338 to 9682.927 in 10 epochs\n",
      "\n",
      "  EPOCH 1 \n",
      "    Train loss: 9603.833\n",
      "  EPOCH 2 \n",
      "    Train loss: 9530.271\n",
      "  EPOCH 3 \n",
      "    Train loss: 9461.524\n",
      "  EPOCH 4 \n",
      "    Train loss: 9396.978\n",
      "  EPOCH 5 \n",
      "    Train loss: 9336.105\n",
      "  EPOCH 6 \n",
      "    Train loss: 9278.445\n",
      "  EPOCH 7 \n",
      "    Train loss: 9223.598\n",
      "  EPOCH 8 \n",
      "    Train loss: 9171.211\n",
      "  EPOCH 9 \n",
      "    Train loss: 9120.974\n",
      "  EPOCH 10 \n",
      "    Train loss: 9072.611\n",
      "\n",
      "Note how the loss continued decreasing from where it had left off\n"
     ]
    }
   ],
   "source": [
    "# To train, the tensor network cores must first be made trainable\n",
    "base_tn = cc.make_trainable(base_tn)\n",
    "\n",
    "# continuous_optim requires the following as input:\n",
    "# (1) A tensor network model, base_tn\n",
    "# (2) A target dataset, in this case just goal_tn\n",
    "# (3) A loss function of the form loss_fun(base_tn, batch), where \n",
    "#     batch is a minibatch of training data (just goal_tn here)\n",
    "# The choice of (2)+(3) fully determines the learning task, with other \n",
    "# problems taking regular datasets as inputs\n",
    "\n",
    "# For tensor recovery, use tensor_recovery_loss from core_code module\n",
    "# Remember, goal_tn is the weirdly-shaped TN and base_tn has rank 3\n",
    "loss_fun = cc.tensor_recovery_loss\n",
    "trained_tn, init_loss, final_loss = cc.continuous_optim(base_tn, goal_tn, \n",
    "                                                        loss_fun)\n",
    "print(f\"Train loss went from {init_loss:.3f} to {final_loss:.3f} in 10 epochs\\n\")\n",
    "\n",
    "# Note that trained_tn gives the model after training, which will be \n",
    "# needed for discrete optimization algorithm. To continue training the \n",
    "# trained model, just run the same code above, but with trained_tn as input\n",
    "_, init_loss, final_loss = cc.continuous_optim(trained_tn, goal_tn, loss_fun)\n",
    "print(\"Note how the loss continued decreasing from where it had left off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customizing the Continuous Optimization Procedure\n",
    "\n",
    "Although we can't directly tweak the continuous_optim code for different problem types, we still have a lot of flexibility owing to the `other_args` argument (a dictionary). Let's explore the current options for `other_args`, and more options can be added later if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EPOCH 1 \n",
      "    Train loss: 10856.338\n",
      "  EPOCH 2 \n",
      "    Train loss: 10659.803\n",
      "\n",
      "Does Adam do any better than SGD? Let's find out!\n",
      "  EPOCH 1 \n",
      "    Train loss: 10856.338\n",
      "  EPOCH 2 \n",
      "    Train loss: 10849.237\n",
      "  EPOCH 3 \n",
      "    Train loss: 10842.151\n",
      "  EPOCH 4 \n",
      "    Train loss: 10835.081\n",
      "  EPOCH 5 \n",
      "    Train loss: 10828.027\n",
      "  EPOCH 6 \n",
      "    Train loss: 10820.989\n",
      "  EPOCH 7 \n",
      "    Train loss: 10813.968\n",
      "  EPOCH 8 \n",
      "    Train loss: 10806.963\n",
      "  EPOCH 9 \n",
      "    Train loss: 10799.975\n",
      "  EPOCH 10 \n",
      "    Train loss: 10793.005\n",
      "\n",
      "Nope\n",
      "\n",
      "Beginning silent training...\n",
      "Silent training finished\n",
      "Train loss went from 10856.338 to 9682.927 in 10 epochs\n",
      "\n",
      "For recovery, it's useful to go through the dataset many times per epoch\n",
      "Note: This is just a trick to avoid excessive printing\n",
      "  EPOCH 1 (10 reps)\n",
      "    Train loss: 10188.294\n",
      "  EPOCH 2 (10 reps)\n",
      "    Train loss: 9319.555\n",
      "  EPOCH 3 (10 reps)\n",
      "    Train loss: 8833.026\n",
      "  EPOCH 4 (10 reps)\n",
      "    Train loss: 8426.848\n",
      "  EPOCH 5 (10 reps)\n",
      "    Train loss: 7988.277\n",
      "  EPOCH 6 (10 reps)\n",
      "    Train loss: 7447.364\n",
      "  EPOCH 7 (10 reps)\n",
      "    Train loss: 6757.394\n",
      "  EPOCH 8 (10 reps)\n",
      "    Train loss: 5927.487\n",
      "  EPOCH 9 (10 reps)\n",
      "    Train loss: 5070.428\n",
      "  EPOCH 10 (10 reps)\n",
      "    Train loss: 4321.798\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 epochs is the default, but you can change this\n",
    "cc.continuous_optim(base_tn, goal_tn, loss_fun, epochs=2)\n",
    "\n",
    "# Feeding a dictionary as other_args arg of continuous_optim lets you \n",
    "# control the optimizer (chosen from torch.optim), and lots else\n",
    "print(\"Does Adam do any better than SGD? Let's find out!\")\n",
    "adam = {'optim': 'Adam'}   # Default: 'SGD'\n",
    "\n",
    "_ = cc.continuous_optim(base_tn, goal_tn, loss_fun, other_args=adam)\n",
    "print(\"Nope\\n\")\n",
    "\n",
    "# Other important arguments are learning rate and batch size, shown\n",
    "# below with their default values\n",
    "other_args = {'lr':    1e-3,\n",
    "              'batch': 100}\n",
    "\n",
    "# You can also run optimization silently via the `print` argument\n",
    "silent = {'print': False}      # Default: True\n",
    "print(\"Beginning silent training...\")\n",
    "_, init_loss, final_loss = cc.continuous_optim(base_tn, goal_tn, loss_fun, \n",
    "                                               other_args=silent)\n",
    "print(\"Silent training finished\")\n",
    "print(f\"Train loss went from {init_loss:.3f} to {final_loss:.3f} in 10 epochs\\n\")\n",
    "\n",
    "# For tensor recovery, there is only one item in our dataset (goal_tn), \n",
    "# leading to only one gradient step per epoch. Using the `reps` argument \n",
    "# can reduce printing by going through training data many times per epoch\n",
    "print(\"For recovery, it's useful to go through the dataset many times per epoch\")\n",
    "print(\"Note: This is just a trick to avoid excessive printing\")\n",
    "lotsa_reps = {'reps': 10}   # Default: 1\n",
    "_ = cc.continuous_optim(base_tn, goal_tn, loss_fun, other_args=lotsa_reps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression of Scalar-Valued Function\n",
    "\n",
    "Continuous optimization for function regression works exactly the same as for tensor recovery, but with a different loss function and target dataset format. To generate this data from a TN, use `generate_regression_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  EPOCH 1 \n",
      "    Train loss: 9357.021\n",
      "  EPOCH 2 \n",
      "    Train loss: 8719.072\n",
      "  EPOCH 3 \n",
      "    Train loss: 8234.186\n",
      "  EPOCH 4 \n",
      "    Train loss: 7839.052\n",
      "  EPOCH 5 \n",
      "    Train loss: 7498.393\n",
      "  EPOCH 6 \n",
      "    Train loss: 7191.073\n",
      "  EPOCH 7 \n",
      "    Train loss: 6903.963\n",
      "  EPOCH 8 \n",
      "    Train loss: 6629.097\n",
      "  EPOCH 9 \n",
      "    Train loss: 6362.363\n",
      "  EPOCH 10 \n",
      "    Train loss: 6102.885\n",
      "\n",
      "Same training process, but with validation data\n",
      "    Val. loss:  10309.398\n",
      "  EPOCH 1 \n",
      "    Train loss: 9357.021\n",
      "    Val. loss:  10387.790\n",
      "  EPOCH 2 \n",
      "    Train loss: 8719.072\n",
      "    Val. loss:  10455.533\n",
      "  EPOCH 3 \n",
      "    Train loss: 8234.186\n",
      "    Val. loss:  10514.973\n",
      "  EPOCH 4 \n",
      "    Train loss: 7839.052\n",
      "    Val. loss:  10568.012\n",
      "  EPOCH 5 \n",
      "    Train loss: 7498.393\n",
      "    Val. loss:  10616.134\n",
      "  EPOCH 6 \n",
      "    Train loss: 7191.073\n",
      "    Val. loss:  10660.522\n",
      "  EPOCH 7 \n",
      "    Train loss: 6903.963\n",
      "    Val. loss:  10702.162\n",
      "  EPOCH 8 \n",
      "    Train loss: 6629.097\n",
      "    Val. loss:  10741.914\n",
      "  EPOCH 9 \n",
      "    Train loss: 6362.363\n",
      "    Val. loss:  10780.555\n",
      "  EPOCH 10 \n",
      "    Train loss: 6102.885\n",
      "    Val. loss:  10818.779\n",
      "\n",
      "    Val. loss:  10309.398\n",
      "  EPOCH 1 \n",
      "    Train loss: 9357.021\n",
      "    Val. loss:  10387.790\n",
      "  EPOCH 2 \n",
      "    Train loss: 8719.072\n",
      "    Val. loss:  10455.533\n",
      "Early stopping condition reached\n",
      "\n",
      "Lowest validation error was 10309.398\n"
     ]
    }
   ],
   "source": [
    "# The generate_regression_data function takes in a target TN and a dataset\n",
    "# size, and produces a pair of random inputs and associated (noisy) outputs\n",
    "num_train = 10000\n",
    "# The noise argument sets the StDev of Gaussian noise added to outputs\n",
    "# (default: 1e-6)\n",
    "train_data = cc.generate_regression_data(goal_tn, num_train, noise=1e-6)\n",
    "\n",
    "# For regression, use cc.regression_loss\n",
    "loss_fun = cc.regression_loss\n",
    "_ = cc.continuous_optim(base_tn, train_data, loss_fun)\n",
    "\n",
    "# Since we're doing machine learning, it's good to have a held-out validation \n",
    "# set to determine loss, early stopping, etc. This is easy to do\n",
    "num_val = 1000\n",
    "val_data = cc.generate_regression_data(goal_tn, num_val)\n",
    "print(\"Same training process, but with validation data\")\n",
    "_ = cc.continuous_optim(base_tn, train_data, loss_fun, val_data=val_data)\n",
    "\n",
    "# It appears there's a lot of overfitting going on! In this case, we can\n",
    "# use the validation loss to choose the stopping time, which is done by\n",
    "# setting epochs=None in continuous_optim\n",
    "_, _, best_loss = cc.continuous_optim(base_tn, train_data, loss_fun, \n",
    "                                      val_data=val_data, epochs=None)\n",
    "print(f\"Lowest validation error was {best_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weird Behavior in Regression\n",
    "\n",
    "Strangely, the validation error for function regression is increasing with time, which should not be happening. I played around with different target tensors, and this doesn't always occur. However, the validation loss is consistently much larger than the training loss for function regression. Some experimentation is needed here!\n",
    "\n",
    "Another issue comes at the end of training, where the loss seems to oscillate a lot around a final value. Using a learning rate that decreases throughout the training would likely be beneficial, and could improve the problem mentioned above. This isn't currently supported, but would be easy to implement by passing in scheduler-specific arguments via `other_args`, and initializing one of the schedulers in `torch.optim.lr_scheduler`.\n",
    "\n",
    "Lastly, here's a quick example of how you can set up an experiment to test some behavior. I'm looking at the relative performance of different optimizers, as tested by how much they decrease the tensor recovery loss after 200 epochs of training. Surprisingly, a lot of the fancy optimizers do very poorly, suggesting that what works for gradient descent with neural networks might not be a good fit for tensor networks (at least, the fully-connected tensor networks we have here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing optimizers...\n",
      "  Adadelta\n",
      "  Adagrad\n",
      "  Adam\n",
      "  AdamW\n",
      "  Adamax\n",
      "  RMSprop\n",
      "  Rprop\n",
      "  SGD\n",
      "The ranking in loss decrease is (max is 100%)\n",
      "  Adadelta: 0.06% decrease\n",
      "  Adagrad:  1.69% decrease\n",
      "  Adamax:   10.11% decrease\n",
      "  Adam:     10.20% decrease\n",
      "  AdamW:    10.29% decrease\n",
      "  RMSprop:  14.90% decrease\n",
      "  SGD:      94.69% decrease\n",
      "  Rprop:    99.95% decrease\n"
     ]
    }
   ],
   "source": [
    "# Example mini-experiment to compare performance of different optimizers\n",
    "\n",
    "candidate_optims = ['Adadelta', 'Adagrad', 'Adam', 'AdamW', \n",
    "                    'Adamax', 'RMSprop', 'Rprop', 'SGD']\n",
    "loss_fun = cc.tensor_recovery_loss\n",
    "percent_dec = {}\n",
    "my_args = {'lr': 1e-3, 'print': False}\n",
    "print(\"Testing optimizers...\")\n",
    "for optim in candidate_optims:\n",
    "    print(\"  \" + optim)\n",
    "    my_args['optim'] = optim\n",
    "    _, loss_i, loss_f = cc.continuous_optim(base_tn, goal_tn, loss_fun, \n",
    "                                            epochs=200, other_args=my_args)\n",
    "    percent_dec[optim] = 100 * (loss_i-loss_f) / (loss_i)\n",
    "\n",
    "print(\"The ranking in loss decrease is (max is 100%)\")\n",
    "percent_dec = sorted(percent_dec.items(), key=lambda p_dec: p_dec[1])\n",
    "for optim, p_dec in percent_dec:\n",
    "    print(f\"  {optim+':':<9} {p_dec:.2f}% decrease\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
