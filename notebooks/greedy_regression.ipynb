{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "sys.path.append('..')\n",
    "from core_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add function to core_code that initializes a random tensor with\n",
    "#       prescribed input dimensions and ranks\n",
    "\n",
    "#generate 4-order target tensor\n",
    "#see ipad for supplementary notes on this tensor and its diagram\n",
    "\n",
    "#With Core tensor: 5 nodes\n",
    "#****it will be interestingn to set r5 to r8 to get tucker structure and run the code to see how \n",
    "# the structure compares to tucker decomposition. Tucker decomposition could become another based cased\n",
    "d1 = 3\n",
    "d2 = 3\n",
    "d3 = 3\n",
    "d4 = 3\n",
    "d5 = 3\n",
    "r0 = 2\n",
    "r1 = 3\n",
    "r2 = 4\n",
    "r3 = 3\n",
    "r4 = 2\n",
    "r5 = 3\n",
    "r6 = 2\n",
    "r7 = 2\n",
    "\n",
    "noise = 1e-6\n",
    "#generate at random target tensor\n",
    "\n",
    "A = torch.rand(r1,d1,r0,r6) # + torch.rand(r1,d1,r0,r6)*noise\n",
    "B = torch.rand(r0,d2,r3,r7) # + torch.rand(r0,d2,r3,r7)*noise\n",
    "C = torch.rand(r1,d3,r2,r5) # + torch.rand(r1,d3,r2,r5)*noise\n",
    "D = torch.rand(r3,d4,r2,r4) # + torch.rand(r3,d4,r2,r4)*noise \n",
    "G = torch.rand(r4,r5,r6,r7) # + torch.rand(r4,r5,r6,r7)*noise\n",
    "\n",
    "indxA = 'kilc'\n",
    "indxB = 'ljed'\n",
    "indxC = 'khgb'\n",
    "indxD = 'efga'\n",
    "indxG = 'abcd'   #core tensor\n",
    "\n",
    "indxList = [indxA, indxB, indxC, indxD, indxG]\n",
    "\n",
    "target_Tensor = einSum_Contraction([A,B,C,D,G], [indxA, indxB, indxC, indxD, indxG])\n",
    "\n",
    "\n",
    "#generate training and test sets\n",
    "N=20\n",
    "#p is the percentage of the total number of data N\n",
    "p = int(np.floor(0.65*N))  #so is is 65% of the original data\n",
    "[Xi_data, yi_data, indx] = data_Set_Generator(target_Tensor, N) \n",
    "Xi_train = Xi_data[0:p]\n",
    "yi_train  = yi_data[0:p]\n",
    "Xi_test = Xi_data[p:N+1]\n",
    "yi_test = yi_data[p:N+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters for the experiment (anything you want to feed to the Pytorch optimizer)\n",
    "hyperparams = {'lr': 0.009,\n",
    "               'some_other_param': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the function mentioned in the above cell to initialize our tensor network\n",
    "\n",
    "#following data are used for initializing Greedy method\n",
    "\n",
    "#initilize data\n",
    "r1 = 1\n",
    "r2 = 1\n",
    "r3 = 1\n",
    "r4 = 1\n",
    "r5 = 1\n",
    "r6 = 1\n",
    "r7 = 1\n",
    "r8 = 1\n",
    "\n",
    "#initialize tensor \n",
    "A_0 = torch.rand(r2,d1,r1,r7)\n",
    "B_0 = torch.rand(r1,d2,r4,r8)\n",
    "C_0 = torch.rand(r2,d3,r3,r6) \n",
    "D_0 = torch.rand(r4,d4,r3,r5) \n",
    "G_0 = torch.rand(r5,r6,r7,r8)\n",
    "\n",
    "#target_Tensor = einSum_Contraction([A_0,B_0,C_0,D_0,G_0], [indxA, indxB, indxC, indxD, indxG])\n",
    "\n",
    "\n",
    "TensorList = [A_0,B_0,C_0,D_0,G_0] \n",
    "TensorList_temp = [A_0,B_0,C_0,D_0,G_0] #TensorList[:]\n",
    "\n",
    "#index list: we use the same indexlist as the ones defiined for target tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7468973a32d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m            \u001b[0;31m# rt = list of ranks: [r1_t,r2_t,r3_t,r4_t,r5_t,r6_t,r7_t,r8_t]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mrt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprintRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensorList_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mnumParam_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetNumParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numParam_greedy='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumParam_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnumParam_temp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tensorCode/core_code.py\u001b[0m in \u001b[0;36mgetNumParams\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetNumParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m#r = list of ranks of a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mnumParam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0md3\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnumParam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'd1' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: Package up code for the greedy optimization loop into a standalone function\n",
    "#       in core_tools, where the loss function and target data (among other things) \n",
    "#       are inputs\n",
    "\n",
    "############################### MAIN LOOP FOR GREEDY #####################\n",
    "\n",
    "#Initialize data\n",
    "iterNum=100   #500\n",
    "Lost_star = 1e12  #set it to be any large number\n",
    "check = 1\n",
    "maxParam = d1*d2*d3*d4\n",
    "numParam = -1\n",
    "paramKey = -1\n",
    "G = []\n",
    "\n",
    "for k in range(5):\n",
    "    if paramKey == 1:\n",
    "        break\n",
    "    for i in range(len(TensorList_temp)):\n",
    "        if paramKey == 1:\n",
    "            break\n",
    "        for j in range(i,len(TensorList_temp)):\n",
    "            if paramKey == 1:\n",
    "                break\n",
    "            if i==j:\n",
    "                continue\n",
    "            #print(i,j)\n",
    "            #increase the ranks of the tensors\n",
    "            [TensorList_temp[i],TensorList_temp[j]] = increaseRank(TensorList_temp[i], TensorList_temp[j],  indxList[i], indxList[j])            \n",
    "            #check num of paramters for the newly updated ranks\n",
    "           # rt = list of ranks: [r1_t,r2_t,r3_t,r4_t,r5_t,r6_t,r7_t,r8_t]\n",
    "            rt = printRank(TensorList_temp)\n",
    "            numParam_temp = getNumParams(rt)\n",
    "            print('numParam_greedy=', numParam_temp)\n",
    "            if numParam_temp > maxParam:\n",
    "                paramKey = 1\n",
    "                print('Max number of parameters exceeded. Current Param = ', numParam_temp, 'and max Param allowed = ', maxParam)\n",
    "                print('program finish ')\n",
    "                break\n",
    "            #solve continuous part\n",
    "            targetData = (yi_train, Xi_train)\n",
    "            [TensorList_temp, indxList, LostList] = solve_Continuous(targetData, TensorList_temp, indxList, iterNum,\n",
    "                                                                     computeLoss_Regression, hyperparams)\n",
    "            printRank(TensorList_temp)\n",
    "            #store the optimal value for a given point around its neighbour\n",
    "            if Lost_star > LostList[-1]: \n",
    "                    indx_star = [i,j]\n",
    "                    TensorList_star = TensorList_temp[:]\n",
    "                    indxList_star = indxList[:]  \n",
    "                    Lost_star = LostList[-1]  \n",
    "                    r_greedy = printRank(TensorList_star)\n",
    "                    print('r_greedy = ', r_greedy)\n",
    "                    numParam = getNumParams(r_greedy )\n",
    "                    check = -1\n",
    "            elif Lost_star <= LostList[-1]:\n",
    "                print('k = ', k, 'Prev Loss = ', Lost_star, 'Current Loss = ', LostList[-1])\n",
    "                print('Loss previous is less than current, so no rank update is made at this iteration')\n",
    "                print('total number of parameters for best chosen rank = ', numParam)\n",
    "         \n",
    "            #Reset TensorList_temp to continue with greedy at another point\n",
    "            TensorList_temp = TensorList[:] #set back to previous position to continue with greedy search        \n",
    "    #update parameters   \n",
    "    TensorList_temp = TensorList_star[:]    #everything is behaving as expected\n",
    "    indxList = indxList_star[:]     # don't really need to update this cause these don't really change\n",
    "    TensorList_greedy   = TensorList_star[:]  #everything is behaving as expected\n",
    "    G.append(Lost_star)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GREEDY script continue\n",
    "#TensorList contains decomposed block of tensors. We use einsum to combine them into one \n",
    "#big tensor Tapprox_greedy\n",
    "Tapprox_greedy = einSum_Contraction(TensorList_greedy, indxList)  #TensorList_Greedy = TensorList\n",
    "print('Tapprox_greedy.shape = ', Tapprox_greedy.shape)\n",
    "\n",
    "#generate yi_approx = tensorApprox_star * X_i. Plot this set of yi_approx, with yi\n",
    "indxlist_greedy = ['abcd', 'abcd']   # abcd =  = d1*d2*d3*d4 i.e. each alphabet represents each dimension ot target tensor\n",
    "yi_approxSet_greedy = getYi_set(Xi_test, Tapprox_greedy, indxlist_greedy)\n",
    "print('len(yi_approxSet_greedy) = ', len(yi_approxSet_greedy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # plot for all the greedy loops\n",
    "    #import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(G, 'o')\n",
    "plt.show()\n",
    "print('greedy: G = ', G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Package up code for the stochastic optimization loop into a standalone function\n",
    "#       in core_tools, where the loss function and target data (among other things) \n",
    "#       are inputs\n",
    "\n",
    "#***********************STOCHASTIC APPROACH MAIN******************************************\n",
    "\n",
    "#The following main program assigns random values to each of the 8 ranks and computes the continuos optimization\n",
    "#part. It repeats 8 times (8 trials) and selects the trial with the smallest lost.\n",
    "\n",
    "#initilize data\n",
    "iterNum=100   #500\n",
    "maxParam = d1*d2*d3*d4\n",
    "#print('maxParam = ', maxParam) \n",
    "L = []\n",
    "#generate sequence of random ranks between 1 to 8\n",
    "numRank = 8\n",
    "d=4  #highest dimension you want to explore\n",
    "max_numShots = 15  #number of random trials you want to perform\n",
    "dmax = max(d1,d2,d3,d4)\n",
    "numShots = 0\n",
    "LostList_prev = 1e12\n",
    "\n",
    "#for i in range(numShots):\n",
    "while numShots <max_numShots:\n",
    "    r_stoch = get_RandomSeqence(numRank,d)\n",
    "    #initialize tensor \n",
    "    A_0 = torch.rand(r_stoch[1],d1,r_stoch[0],r_stoch[6])\n",
    "    B_0 = torch.rand(r_stoch[0],d2,r_stoch[3],r_stoch[7])\n",
    "    C_0 = torch.rand(r_stoch[1],d3,r_stoch[2],r_stoch[5])\n",
    "    D_0 = torch.rand(r_stoch[3],d4,r_stoch[2],r_stoch[4]) \n",
    "    G_0 = torch.rand(r_stoch[4],r_stoch[5],r_stoch[6],r_stoch[7])\n",
    "    TensorList_temp = [A_0,B_0,C_0,D_0,G_0] #TensorList[:]   \n",
    "    #the following if loop checks the number of elements of each A_0, ..., G_0\n",
    "    #does not exceed the total number elements of target tensor\n",
    "    #numParamList = getNumParams(r_stoch)\n",
    "    #if maxParam > numParamList[0] and maxParam > numParamList[1] and maxParam > numParamList[2] and maxParam > numParamList[3] and maxParam > numParamList[4]:\n",
    "    if maxParam > getNumParams(r_stoch):\n",
    "        targetData = (yi_train, Xi_train)\n",
    "        [TensorList_temp, indxList, LostList] = solve_Continuous(targetData, TensorList_temp, indxList, iterNum,\n",
    "                                                                 computeLoss_Regression, hyperparams)\n",
    "        L.append(LostList[-1])\n",
    "        numShots += 1\n",
    "        #the following if-loop stores the approx tensor that gives the smallest loss\n",
    "        if LostList[-1] <LostList_prev:\n",
    "            tensorApprox_star = TensorList_temp[:]\n",
    "            LostList_prev = LostList[-1]\n",
    "            r_Star = r_stoch       \n",
    "print('Stochastic Method: best rank r = ', r_Star, 'with loss = ', LostList_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...STOCHASTIC SCRIPT CONTINUE\n",
    "#tensorApprox_star consists of blocks of tensor. We take tensor product of all these\n",
    "#blocks to get one big block: TensorApprox_Stoch\n",
    "indxList = [indxA, indxB, indxC, indxD, indxG]  #use the exact same indxList as the ones above\n",
    "TensorApprox_Stoch = einSum_Contraction([tensorApprox_star[0],tensorApprox_star[1],tensorApprox_star[2],tensorApprox_star[3],tensorApprox_star[4]], [indxA, indxB, indxC, indxD, indxG])\n",
    "\n",
    "#generate yi_approx = tensorApprox_star * X_i. \n",
    "indxlist_stoch = [indx, indx]   # indx_Xi = indx_W = indx. \n",
    "yi_approxSet_stoch = getYi_set(Xi_test, TensorApprox_Stoch, indxlist_stoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Package up code for the random walk optimization loop into a standalone function\n",
    "#       in core_tools, where the loss function and target data (among other things) \n",
    "#       are inputs\n",
    "\n",
    "#################################Random Walk(RW) main###############################\n",
    "maxParam = d1*d2*d3*d4\n",
    "iterNum = 100\n",
    "#initialize the edges to 1\n",
    "r_RW = [1,1,1,1,1,1,1,1]\n",
    "numParam_RW = getNumParams(r_RW)\n",
    "Lost_starRW = 1e12  #set it to be any large number\n",
    "G_RW = []\n",
    "while maxParam > numParam_RW:\n",
    "    print('r_RW =', r_RW)\n",
    "    r_RW = get_Next_randomEdge(r_RW)\n",
    "    numParam_RW = getNumParams(r_RW)\n",
    "    #initialize tensor \n",
    "    if maxParam > numParam_RW:\n",
    "        A_0 = torch.rand(r_RW[1],d1,r_RW[0],r_RW[6])\n",
    "        B_0 = torch.rand(r_RW[0],d2,r_RW[3],r_RW[7])\n",
    "        C_0 = torch.rand(r_RW[1],d3,r_RW[2],r_RW[5])\n",
    "        D_0 = torch.rand(r_RW[3],d4,r_RW[2],r_RW[4]) \n",
    "        G_0 = torch.rand(r_RW[4],r_RW[5],r_RW[6],r_RW[7])\n",
    "        TensorList_RW = [A_0,B_0,C_0,D_0,G_0]\n",
    "        targetData = (yi_train, Xi_train)\n",
    "        [TensorList_RW, indxList, LostList_RW] = solve_Continuous(targetData, TensorList_RW, indxList, iterNum,\n",
    "                                                                  computeLoss_Regression, hyperparams)\n",
    "        G_RW.append(LostList_RW[-1])\n",
    "\n",
    "print('Random Walk: numParam = ', numParam_RW, 'maxNumParam of target tensor = ', maxParam)\n",
    "print('G_RW = ', G_RW)\n",
    "\n",
    "#take the block tensors and combine them to form 1 big tensor: TensorApprox_RW \n",
    "indxList = [indxA, indxB, indxC, indxD, indxG]  #use the exact same indxList as the ones above\n",
    "TensorApprox_RW = einSum_Contraction([TensorList_RW[0], TensorList_RW[1], TensorList_RW[2], TensorList_RW[3], TensorList_RW[4]], [indxA, indxB, indxC, indxD, indxG])\n",
    "indxlist_RW = ['abcd', 'abcd']   # abcd =  = d1*d2*d3*d4 i.e. each alphabet represents each dimension ot target tensor\n",
    "yi_approxSet_RW = getYi_set(Xi_test, TensorApprox_RW, indxlist_RW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##PLOT FOR RANDOM WALK. X-AXIS = total number of steps and Y-axis = LostList[-1]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(G_RW, 'og')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we assume yi_actual = yi_train. \n",
    "#The following plot compares the approx yi using different the 3 different approach (greedy, stochastic, random walk)\n",
    "# and compare the approx to the actual data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(yi_approxSet_stoch, 'og')\n",
    "plt.plot(yi_approxSet_greedy, 'ob')\n",
    "plt.plot(yi_approxSet_RW, 'om')\n",
    "plt.plot(yi_test, '*r')   #recall we treat yi_test as our true data\n",
    "plt.title('Greedy, stochastic, randomWalk data vs actual data ',loc='center')\n",
    "plt.show()\n",
    "\n",
    "print('Legend')\n",
    "print('red: actual data')\n",
    "print('green: predicted data using stochastic')\n",
    "print('blue: predicted data using greedy')\n",
    "print('magenta: predicted data using random walk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "RMSE_stoch = sqrt(mean_squared_error(yi_test,yi_approxSet))\n",
    "print('RMSE_stoch = ',RMSE_stoch)\n",
    "\n",
    "RMSE_greedy = sqrt(mean_squared_error(yi_test,yi_approxSet_greedy))\n",
    "print('RMSE_greedy = ',RMSE_greedy) \n",
    "\n",
    "RMSE_RW = sqrt(mean_squared_error(yi_test,yi_approxSet_RW))\n",
    "print('RMSE_randomWalk = ',RMSE_RW) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative error\n",
    "def getRelError(yi_actual,yi_pred):\n",
    "# Both yi_actual,yi_pred are a set of elements\n",
    "    summ = 0\n",
    "    nm = 0\n",
    "    N = len(yi_actual)\n",
    "    for i in range(N):\n",
    "        summ = summ + abs(yi_actual[i] - yi_pred[i])\n",
    "        nm = nm + abs(yi_actual[i])\n",
    "    return summ/nm*100 \n",
    "\n",
    "RE_stoch = getRelError(yi_test,yi_approxSet_stoch)\n",
    "print('stoch Method: % rel. error = ', RE_stoch)\n",
    "\n",
    "RE_greedy = getRelError(yi_test,yi_approxSet_greedy)\n",
    "print('Greedy Method: % rel. error = ', RE_greedy)\n",
    "\n",
    "RE_RW = getRelError(yi_test,yi_approxSet_RW)\n",
    "print('Random Walk Method: % rel. error = ', RE_RW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
