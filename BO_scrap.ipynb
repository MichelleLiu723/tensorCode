{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def get_repeated_Indices(list_of_indices):\n",
    "    #input: string of indices for the tensors. \n",
    "    #output: string of repeated indices. code takes in indices and output only the repeated indices\n",
    "    #Ex: Suppose tensor A has index ijkp, and tensor B has index klpm.\n",
    "    #then get_repeated_Indices('ijkp', 'klpm') will return the following string: 'kp'\n",
    "    myList = list_of_indices\n",
    "    #convert List to string\n",
    "    myString =''.join(myList)\n",
    "    #break the string into indivual list of characters ex.  'abc' ->['a','b', 'c']\n",
    "    myList = list(myString)\n",
    "    #get the repeated frequencies of each indices\n",
    "    my_dict = {i:myList.count(i) for i in myList}\n",
    "    \n",
    "    repeatedList = []\n",
    "    for item in my_dict:\n",
    "        if my_dict[item] > 1:\n",
    "            repeatedList.append(item)\n",
    "    return repeatedList\n",
    "\n",
    "def  remove_Repeated_indices(List_of_indices):\n",
    "    #inputs: tensor indices in the form of string\n",
    "    #output: string of non repeated indicies\n",
    "    #Ex: remove_Repeated_indices('abc', 'cde')\n",
    "    #output of the example would be: 'abde'\n",
    "    \n",
    "    myList = List_of_indices \n",
    "    #turn myList into String: Ex: ['abc','cde'] -> 'abccde'\n",
    "    myString = ''.join(myList)\n",
    "    #turn back into lists again: Exp: from 'abccde' -> ['a','b','c','c','d','e']\n",
    "    myList = list(myString)\n",
    "    repeated_indices = get_repeated_Indices(List_of_indices)\n",
    "    #print('the repeated list of indices are:', repeated_indices)\n",
    "    unique_indices = []\n",
    "    #now we remove repeated indices from myList\n",
    "    for item in myList:\n",
    "        if item not in repeated_indices:\n",
    "            unique_indices.append(item)\n",
    "    uniqueString = ''.join(unique_indices)   \n",
    "    return uniqueString\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def einSum_Contraction(tensorList, indxList):  #<----should rename this to einSum_Contraction to replace old code\n",
    "    #Purpose: this function takes a list of tensors, and list of indices, and indix to contract and uses einstien summation to perform contraction\n",
    "    #ex: tensorList = [tensor1, tensor2, tensor3]\n",
    "    #indxList   = [indx1, indx2, indx3]\n",
    "    myList = []\n",
    "    uniqueIndices = remove_Repeated_indices(indxList)\n",
    "    inputIndices = [indxList]\n",
    "    N = len(indxList)\n",
    "    #myList = [indx1, ',',indx2,',',indx3,'->', uniqueIndices] \n",
    "    for i in range(N - 1):\n",
    "        myList.append(indxList[i])\n",
    "        myList.append(',') \n",
    "    myList.append(indxList[N-1])\n",
    "    myList.append('->')\n",
    "    myList.append(uniqueIndices)\n",
    "    #convert myList to a string: i.e.  [indx1, ',',indx2,',',indx3,'->', uniqueIndices]  - >'ijk,klm,mjp->ilp'\n",
    "    myString = ''.join(myList)\n",
    "    #print('myString = ', myString)\n",
    "    C = torch.einsum(myString, tensorList)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(approxTensor, targetTensor):\n",
    "    cost = torch.norm(approxTensor - targetTensor, 'fro') \n",
    "    return(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padTensor(tensor, pad_axis):\n",
    "    #this is for the discrete optimization\n",
    "    #this function takes a tensor and append an extra dimension of ~ zeros along the specified axis (we call the pad axis)\n",
    "    if pad_axis == -1:\n",
    "        return tensor #don't pad anything\n",
    "    tensorShape = list(tensor.shape)\n",
    "    tensorShape[pad_axis] = 1  #increase the dimension up by 1\n",
    "    zerosPad = torch.rand(tensorShape) *1e-6  #pad with values approx. equal to zero\n",
    "    padded_tensor = torch.cat([tensor, zerosPad], pad_axis)\n",
    "    #print('padded_tensor.shape = ', padded_tensor.shape)\n",
    "    #print('padded_tensor function output = ', padded_tensor)\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increaseRank(Tensor1, Tensor2, indx1, indx2):\n",
    "    # The indx 1 and index2 represents the indices for tensor 1 and 2 respectively. \n",
    "    #There is only one repeated index in the list (indx1, indx2). The repeated index represents the shared edge between\n",
    "    #the two tensors. For ex: ijkl, lmno\n",
    "    alpha = get_repeated_Indices([indx1, indx2])\n",
    "    #convert alpha to string\n",
    "    alpha = ''.join(alpha)\n",
    "    # find the position of the repeated index alpha in indx1 and indx2\n",
    "    padAxes1 = indx1.index(alpha)\n",
    "    padAxes2 = indx2.index(alpha)  \n",
    "    \n",
    "    paddedTensor1 = padTensor(Tensor1, padAxes1)\n",
    "    paddedTensor2 = padTensor(Tensor2, padAxes2)\n",
    "    return  paddedTensor1, paddedTensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 3, 3])\n",
      "torch.Size([3, 2, 4])\n",
      "shapeTargetTensor= torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "#*******************target tensor\n",
    "#Global variables: d1, d2, d3, targetTensor\n",
    "d1 = 2\n",
    "d2 = 3\n",
    "d3 = 4\n",
    "r1 = 2\n",
    "r2 = 3\n",
    "r3 = 2\n",
    "\n",
    "X = torch.rand(d1, r3, r1)\n",
    "Y = torch.rand(r1, d2, r2)\n",
    "Z = torch.rand(r2, r3, d3)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Z.shape)\n",
    "\n",
    "indx0 = 'ijk'\n",
    "indx1 = 'klm'\n",
    "indx2 = 'mjp'\n",
    "\n",
    "target_Tensor = einSum_Contraction([X, Y, Z], [indx0, indx1, indx2])\n",
    "print('shapeTargetTensor=', target_Tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective function\n",
    "def TensorGenerator(r1,r2, r3):\n",
    "    X = torch.rand(d1, r3, r1)\n",
    "    Y = torch.rand(r1, d2, r2)\n",
    "    Z = torch.rand(r2, r3, d3)   \n",
    "    indx0 = 'ijk'\n",
    "    indx1 = 'klm'\n",
    "    indx2 = 'mjp'   \n",
    "    tensorList1 = [X, Y, Z]\n",
    "    indxList1 = [indx0, indx1, indx2]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_Continuous_withInput_rank(r1, r2, r3):\n",
    "#this function is the objective input\n",
    "    \n",
    "#TensorGenerator function is here********************************\n",
    "    X = torch.rand(d1, r3, r1)\n",
    "    Y = torch.rand(r1, d2, r2)\n",
    "    Z = torch.rand(r2, r3, d3)   \n",
    "    indx0 = 'ijk'\n",
    "    indx1 = 'klm'\n",
    "    indx2 = 'mjp'\n",
    "    tensorList = [X, Y, Z]\n",
    "    indxList = [indx0, indx1, indx2] \n",
    "#***************solve_continuous part****************************\n",
    "    len_Tensor = len(tensorList)\n",
    "    len_Indx   = len(indxList)\n",
    "    iterNum = 400\n",
    "    \n",
    "    for i in range(len_Tensor):\n",
    "        tensorList[i] = tensorList[i].detach()\n",
    "        tensorList[i].requires_grad = True\n",
    "\n",
    "    #defines a SGD optimizer to update the parameters\n",
    "    #optimizer = optim.SGD(tensorList lr = 0.001, momentum=0.2)\n",
    "    optimizer = optim.Adam(tensorList, lr=0.009)\n",
    "\n",
    "    for i in range(iterNum):\n",
    "        optimizer.zero_grad()    \n",
    "        tensor_approx = einSum_Contraction(tensorList, indxList)\n",
    "    #################################################################################\n",
    "        loss_fn = computeLoss(tensor_approx, target_Tensor)\n",
    "    #################################################################################\n",
    "        loss_fn.backward()\n",
    "        optimizer.step()                # the new A,B,C will be A_k+1,B_k+1, C_k+1 after optimizer.step \n",
    "    return tensorList, indxList, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      "0 2\n",
      "1 2\n"
     ]
    }
   ],
   "source": [
    "#**********************************good copy. Don't touch this\n",
    "A = X_approx0\n",
    "B = Y_approx0\n",
    "C = Z_approx0\n",
    "\n",
    "indxList = [indx0, indx1, indx2]\n",
    "TensorList_temp = [A, B, C]\n",
    "iterNum=500\n",
    "Lost_star = 5\n",
    "\n",
    "\n",
    "for i in range (len(TensorList_temp)):\n",
    "    for j in range(i,len(TensorList_temp)):\n",
    "        if i==j:\n",
    "            continue\n",
    "        print(i,j)\n",
    "        [TensorList_temp[i],TensorList_temp[j]] =  increaseRank(TensorList_temp[i], TensorList_temp[j],  indxList[i], indxList[j])\n",
    "        [TensorList_temp, indxList, LostList] = solve_Continuous_withInput_rank(r1, r2, r3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
