{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see notes in blue binder for documents of this. \n",
    "#code was tested on three tensors: A-B-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def get_repeated_Indices(list_of_indices):\n",
    "    #input: string of indices for the tensors. \n",
    "    #output: string of repeated indices. code takes in indices and output only the repeated indices\n",
    "    #Ex: Suppose tensor A has index ijkp, and tensor B has index klpm.\n",
    "    #then get_repeated_Indices('ijkp', 'klpm') will return the following string: 'kp'\n",
    "    myList = list_of_indices\n",
    "    #convert List to string\n",
    "    myString =''.join(myList)\n",
    "    #break the string into indivual list of characters ex.  'abc' ->['a','b', 'c']\n",
    "    myList = list(myString)\n",
    "    #get the repeated frequencies of each indices\n",
    "    my_dict = {i:myList.count(i) for i in myList}\n",
    "    \n",
    "    repeatedList = []\n",
    "    for item in my_dict:\n",
    "        if my_dict[item] > 1:\n",
    "            repeatedList.append(item)\n",
    "    return repeatedList\n",
    "\n",
    "def  remove_Repeated_indices(List_of_indices):\n",
    "    #inputs: tensor indices in the form of string\n",
    "    #output: string of non repeated indicies\n",
    "    #Ex: remove_Repeated_indices('abc', 'cde')\n",
    "    #output of the example would be: 'abde'\n",
    "    \n",
    "    myList = List_of_indices \n",
    "    #turn myList into String: Ex: ['abc','cde'] -> 'abccde'\n",
    "    myString = ''.join(myList)\n",
    "    #turn back into lists again: Exp: from 'abccde' -> ['a','b','c','c','d','e']\n",
    "    myList = list(myString)\n",
    "    repeated_indices = get_repeated_Indices(List_of_indices)\n",
    "    #print('the repeated list of indices are:', repeated_indices)\n",
    "    unique_indices = []\n",
    "    #now we remove repeated indices from myList\n",
    "    for item in myList:\n",
    "        if item not in repeated_indices:\n",
    "            unique_indices.append(item)\n",
    "    uniqueString = ''.join(unique_indices)   \n",
    "    return uniqueString\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def einSum_Contraction(tensorList, indxList):  #<----should rename this to einSum_Contraction to replace old code\n",
    "    #Purpose: this function takes a list of tensors, and list of indices, and indix to contract and uses einstien summation to perform contraction\n",
    "    #ex: tensorList = [tensor1, tensor2, tensor3]\n",
    "    #indxList   = [indx1, indx2, indx3]\n",
    "    myList = []\n",
    "    uniqueIndices = remove_Repeated_indices(indxList)\n",
    "    inputIndices = [indxList]\n",
    "    N = len(indxList)\n",
    "    #myList = [indx1, ',',indx2,',',indx3,'->', uniqueIndices] \n",
    "    for i in range(N - 1):\n",
    "        myList.append(indxList[i])\n",
    "        myList.append(',') \n",
    "    myList.append(indxList[N-1])\n",
    "    myList.append('->')\n",
    "    myList.append(uniqueIndices)\n",
    "    #convert myList to a string: i.e.  [indx1, ',',indx2,',',indx3,'->', uniqueIndices]  - >'ijk,klm,mjp->ilp'\n",
    "    myString = ''.join(myList)\n",
    "    #print('myString = ', myString)\n",
    "    C = torch.einsum(myString, tensorList)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLoss(approxTensor, targetTensor):\n",
    "    cost = torch.norm(approxTensor - targetTensor, 'fro') \n",
    "    return(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padTensor(tensor, pad_axis):\n",
    "    #this is for the discrete optimization\n",
    "    #this function takes a tensor and append an extra dimension of ~ zeros along the specified axis (we call the pad axis)\n",
    "    if pad_axis == -1:\n",
    "        return tensor #don't pad anything\n",
    "    tensorShape = list(tensor.shape)\n",
    "    tensorShape[pad_axis] = 1  #increase the dimension up by 1\n",
    "    zerosPad = torch.rand(tensorShape) *1e-6  #pad with values approx. equal to zero\n",
    "    padded_tensor = torch.cat([tensor, zerosPad], pad_axis)\n",
    "    #print('padded_tensor.shape = ', padded_tensor.shape)\n",
    "    #print('padded_tensor function output = ', padded_tensor)\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increaseRank(Tensor1, Tensor2, indx1, indx2):\n",
    "    # The indx 1 and index2 represents the indices for tensor 1 and 2 respectively. \n",
    "    #There is only one repeated index in the list (indx1, indx2). The repeated index represents the shared edge between\n",
    "    #the two tensors. For ex: ijkl, lmno\n",
    "    alpha = get_repeated_Indices([indx1, indx2])\n",
    "    #convert alpha to string\n",
    "    alpha = ''.join(alpha)\n",
    "    # find the position of the repeated index alpha in indx1 and indx2\n",
    "    padAxes1 = indx1.index(alpha)\n",
    "    padAxes2 = indx2.index(alpha)  \n",
    "    \n",
    "    paddedTensor1 = padTensor(Tensor1, padAxes1)\n",
    "    paddedTensor2 = padTensor(Tensor2, padAxes2)\n",
    "    return  paddedTensor1, paddedTensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_Continuous(target_Tensor, tensorList, indxList, iterNum):\n",
    "#input: list of tensors and their corresponding indices\n",
    "#Goal: The purpose of this function is to solve the innerloop of the optimization for the problem\n",
    "    len_Tensor = len(tensorList)\n",
    "    len_Indx   = len(indxList)\n",
    "    \n",
    "\n",
    "    for i in range(len_Tensor):\n",
    "        tensorList[i] = tensorList[i].detach()\n",
    "        tensorList[i].requires_grad = True\n",
    "\n",
    "    #defines a SGD optimizer to update the parameters\n",
    "    #optimizer = optim.SGD(tensorList lr = 0.001, momentum=0.2)\n",
    "    optimizer = optim.Adam(tensorList, lr=0.009)\n",
    "\n",
    "    #initialize parameters      \n",
    "    LostList = []              #use this to plot the lost function\n",
    "\n",
    "    for i in range(iterNum):\n",
    "        optimizer.zero_grad()\n",
    "        tensor_approx = einSum_Contraction(tensorList, indxList)\n",
    "        loss_fn = computeLoss(tensor_approx, target_Tensor)\n",
    "        loss_fn.backward()\n",
    "        optimizer.step()                # the new A,B,C will be A_k+1,B_k+1, C_k+1 after optimizer.step \n",
    "        LostList.append(loss_fn)\n",
    "    return tensorList, indxList, LostList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgPoints(list1, num_points):\n",
    "    #num_points = number of elements from list1 that you want to take the average of\n",
    "    n = len(list1)\n",
    "    avg = sum(list1[n-num_points:n])/num_points\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 2])\n",
      "torch.Size([2, 3, 3])\n",
      "torch.Size([3, 2, 4])\n",
      "shapeTargetTensor= torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "#generate target tensor of rank 1\n",
    "X =  torch.Tensor([[[1]],[[2]]])\n",
    "Y =  torch.Tensor([[[3],[4],[5]]])\n",
    "Z =  torch.Tensor([[[6,7,8,9]]])\n",
    "\n",
    "d1 = 2\n",
    "d2 = 3\n",
    "d3 = 4\n",
    "r1 = 2\n",
    "r2 = 3\n",
    "r3 = 2\n",
    "\n",
    "X = torch.rand(d1, r3, r1)\n",
    "Y = torch.rand(r1, d2, r2)\n",
    "Z = torch.rand(r2, r3, d3)\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(Z.shape)\n",
    "\n",
    "indx0 = 'ijk'\n",
    "indx1 = 'klm'\n",
    "indx2 = 'mjp'\n",
    "\n",
    "target_Tensor = einSum_Contraction([X, Y, Z], [indx0, indx1, indx2])\n",
    "print('shapeTargetTensor=', target_Tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initilize\n",
    "r1 = 1\n",
    "r2 = 1\n",
    "r3 = 1\n",
    "#generate at random target tensor\n",
    "X_approx0 = torch.rand(d1,r3,r1)*1\n",
    "X_approx0 = X_approx0/torch.norm(X_approx0, 'fro') \n",
    "Y_approx0 = torch.rand(r1,d2,r2)*1\n",
    "Y_approx0 = Y_approx0/torch.norm(Y_approx0, 'fro') \n",
    "Z_approx0 = torch.rand(r2,r3,d3)*1\n",
    "Z_approx0 = Z_approx0/torch.norm(Z_approx0, 'fro') \n",
    "\n",
    "##this intialize tensor close to target tensor. This is to confirm the loss function is low as eplected\n",
    "#noise = 0.000000000001\n",
    "#X_approx0 =  torch.Tensor([[[1]],[[2]]]) + torch.rand(2,1,1)*noise  #need to check it is from normal distribution\n",
    "#Y_approx0 =  torch.Tensor([[[3],[4],[5]]])+ torch.rand(1,3,1)*noise\n",
    "#Z_approx0 =  torch.Tensor([[[6,7,8,9]]])+ torch.rand(1,1,4)*noise\n",
    "\n",
    "indx0 = 'ijk'\n",
    "indx1 = 'kmn'\n",
    "indx2 = 'njp'\n",
    "indxList = [indx0, indx1, indx2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printRank(TensorList):\n",
    "    [d1, r3, r1] = (TensorList[0].shape) #tensor X dimension\n",
    "    [r1, d2, r2] = (TensorList[1].shape) #tensor Y dimension\n",
    "    print(' [r1, r2, r3] = ', '[', r1, ',', r2, ',', r3, ']')\n",
    "    return r1,r2,r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNumParams(r1,r2,r3):\n",
    "    numParam = d1*r1*r3 + r1*d2*r2 + r2*r3*d3\n",
    "    #print('d1*r1*r3 = ', d1*r1*r3)\n",
    "    #print('r1*d2*r2', r1*d2*r2)\n",
    "    #print('r2*r3*d3', r2*r3*d3)\n",
    "    return numParam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n",
      " [r1, r2, r3] =  [ 2 , 1 , 1 ]\n",
      "num of parameters for recently updated ranks=  14\n",
      "Currently evaluating rank:\n",
      " [r1, r2, r3] =  [ 2 , 1 , 1 ]\n",
      "Lost* updated: Lost_star previous = 5 Lost_star current = tensor(0.7328, grad_fn=<NormBackward0>)\n",
      "current Best rank:\n",
      " [r1, r2, r3] =  [ 2 , 1 , 1 ]\n",
      "total number of parameters for best chosen ranks =  14\n",
      "***************************************************************\n",
      "0 2\n",
      " [r1, r2, r3] =  [ 1 , 1 , 2 ]\n",
      "num of parameters for recently updated ranks=  15\n",
      "Currently evaluating rank:\n",
      " [r1, r2, r3] =  [ 1 , 1 , 2 ]\n",
      "Prev Loss =  tensor(0.7328, grad_fn=<NormBackward0>) Current Loss =  tensor(0.9622, grad_fn=<NormBackward0>)\n",
      "Loss previous is less than current, so no rank update is made at this iteration\n",
      "total number of parameters for best chosen rank =  14\n",
      "***************************************************************\n",
      "1 2\n",
      " [r1, r2, r3] =  [ 1 , 2 , 1 ]\n",
      "num of parameters for recently updated ranks=  16\n",
      "Currently evaluating rank:\n",
      " [r1, r2, r3] =  [ 1 , 2 , 1 ]\n",
      "Prev Loss =  tensor(0.7328, grad_fn=<NormBackward0>) Current Loss =  tensor(0.8271, grad_fn=<NormBackward0>)\n",
      "Loss previous is less than current, so no rank update is made at this iteration\n",
      "total number of parameters for best chosen rank =  14\n",
      "***************************************************************\n",
      "****************************moving to another grid point ***********************************\n",
      "0 1\n",
      " [r1, r2, r3] =  [ 3 , 1 , 1 ]\n",
      "num of parameters for recently updated ranks=  19\n",
      "Currently evaluating rank:\n",
      " [r1, r2, r3] =  [ 3 , 1 , 1 ]\n",
      "Lost* updated: Lost_star previous = tensor(0.7328, grad_fn=<NormBackward0>) Lost_star current = tensor(0.7328, grad_fn=<NormBackward0>)\n",
      "current Best rank:\n",
      " [r1, r2, r3] =  [ 3 , 1 , 1 ]\n",
      "total number of parameters for best chosen ranks =  19\n",
      "***************************************************************\n",
      "0 2\n",
      " [r1, r2, r3] =  [ 2 , 1 , 2 ]\n",
      "num of parameters for recently updated ranks=  22\n",
      "Currently evaluating rank:\n",
      " [r1, r2, r3] =  [ 2 , 1 , 2 ]\n",
      "Lost* updated: Lost_star previous = tensor(0.7328, grad_fn=<NormBackward0>) Lost_star current = tensor(0.5942, grad_fn=<NormBackward0>)\n",
      "current Best rank:\n",
      " [r1, r2, r3] =  [ 2 , 1 , 2 ]\n",
      "total number of parameters for best chosen ranks =  22\n",
      "***************************************************************\n",
      "1 2\n",
      " [r1, r2, r3] =  [ 2 , 2 , 1 ]\n",
      "num of parameters for recently updated ranks=  24\n",
      "Currently evaluating rank:\n",
      " [r1, r2, r3] =  [ 2 , 2 , 1 ]\n",
      "Lost* updated: Lost_star previous = tensor(0.5942, grad_fn=<NormBackward0>) Lost_star current = tensor(0.1092, grad_fn=<NormBackward0>)\n",
      "current Best rank:\n",
      " [r1, r2, r3] =  [ 2 , 2 , 1 ]\n",
      "total number of parameters for best chosen ranks =  24\n",
      "***************************************************************\n",
      "****************************moving to another grid point ***********************************\n",
      "0 1\n",
      " [r1, r2, r3] =  [ 3 , 2 , 1 ]\n",
      "num of parameters for recently updated ranks=  32\n",
      "Max number of parameters exceeded. Current Param =  32 and max Param allowed =  24\n",
      "program finish \n",
      "****************************moving to another grid point ***********************************\n"
     ]
    }
   ],
   "source": [
    "#Here we added a third loop. Each iteration samples a different grid point and its nearby neighbours. It computes the continuous\n",
    "#opt for different rank combinations and stores the one with the least minimum.\n",
    "A = X_approx0\n",
    "B = Y_approx0\n",
    "C = Z_approx0\n",
    "\n",
    "indxList = [indx0, indx1, indx2]\n",
    "TensorList = [A, B, C]\n",
    "TensorList_temp = [A, B, C] #TensorList[:]\n",
    "iterNum=500\n",
    "Lost_star = 5\n",
    "check = 1\n",
    "maxParam = d1*d2*d3\n",
    "numParam = -1\n",
    "paramKey = -1\n",
    "\n",
    "\n",
    "for k in range(5):\n",
    "    if paramKey == 1:\n",
    "        break\n",
    "    for i in range(len(TensorList_temp)):\n",
    "        if paramKey == 1:\n",
    "            break\n",
    "        for j in range(i,len(TensorList_temp)):\n",
    "            if paramKey == 1:\n",
    "                break\n",
    "            if i==j:\n",
    "                continue\n",
    "            print(i,j)\n",
    "            #increase the ranks of the tensors\n",
    "            [TensorList_temp[i],TensorList_temp[j]] = increaseRank(TensorList_temp[i], TensorList_temp[j],  indxList[i], indxList[j])\n",
    "            \n",
    "            #check num of paramters for the newly updated ranks\n",
    "            [r1_t,r2_t,r3_t] = printRank(TensorList_temp)\n",
    "            numParam_temp = getNumParams(r1_t,r2_t,r3_t)\n",
    "            print('num of parameters for recently updated ranks= ', numParam_temp)\n",
    "            if numParam_temp > maxParam:\n",
    "                paramKey = 1\n",
    "                print('Max number of parameters exceeded. Current Param = ', numParam_temp, 'and max Param allowed = ', maxParam)\n",
    "                print('program finish ')\n",
    "                break\n",
    "            #solve continuous part\n",
    "            [TensorList_temp, indxList, LostList] = solve_Continuous(target_Tensor, TensorList_temp, indxList, iterNum)\n",
    "            print('Currently evaluating rank:')\n",
    "            printRank(TensorList_temp)\n",
    "            \n",
    "            #store the optimal value for a given point around its neighbour\n",
    "            if Lost_star > LostList[-1]: #avgLost:  #When have time, take the average of 10 elements as opposed to the min. of last elementLostList[-1]:  #When have time, take the average of 10 elements as opposed to the min. of last element\n",
    "                    indx_star = [i,j]\n",
    "                    TensorList_star = TensorList_temp[:]\n",
    "                    indxList_star = indxList[:]\n",
    "                    print('Lost* updated: Lost_star previous =', Lost_star, 'Lost_star current =', LostList[-1])\n",
    "                    Lost_star = LostList[-1]  \n",
    "                    print('current Best rank:')\n",
    "                    [r1,r2,r3] = printRank(TensorList_star)\n",
    "                    numParam = getNumParams(r1,r2,r3)\n",
    "                    #numParam = d1*r1*r3 + r1*d2*r2 + r2*r3*d3\n",
    "                    #print('d1*r1*r3 = ', d1*r1*r3)\n",
    "                    #print('r1*d2*r2', r1*d2*r2)\n",
    "                    #print('r2*r3*d3', r2*r3*d3)\n",
    "                    print('total number of parameters for best chosen ranks = ', numParam)\n",
    "                    check = -1\n",
    "            elif Lost_star <= LostList[-1]:\n",
    "                print('Prev Loss = ', Lost_star, 'Current Loss = ', LostList[-1])\n",
    "                print('Loss previous is less than current, so no rank update is made at this iteration')\n",
    "                print('total number of parameters for best chosen rank = ', numParam)\n",
    "\n",
    "            \n",
    "            #Reset TensorList_temp to continue with greedy at another point\n",
    "            TensorList_temp = TensorList[:] #set back to previous position to continue with greedy search\n",
    "            print('***************************************************************')  \n",
    "    print('****************************moving to another grid point ***********************************')\n",
    "    #update parameters   \n",
    "    #print('TensorList_star[0] = ', TensorList_star[0])\n",
    "    #print('TensorList_star[1] = ', TensorList_star[1])\n",
    "    #print('TensorList_star[2] = ', TensorList_star[2])\n",
    "    TensorList_temp = TensorList_star[:]    #everything is behaving as expected\n",
    "    #print('TensorList_temp[0] After = ', TensorList_temp[0])\n",
    "    #print('TensorList_temp[1] After = ', TensorList_temp[1])\n",
    "    #print('TensorList_temp[2] After = ', TensorList_temp[2])\n",
    "    #print('indxList_star[2] before = ', indxList_star[2])\n",
    "    indxList = indxList_star[:]     # don't really need to update this cause these don't really change\n",
    "    #print('indxList[2] after = ', indxList[2])\n",
    "    #print('TensorList_star[0] =',  TensorList_star[0])\n",
    "    TensorList  = TensorList_star[:]  #everything is behaving as expected\n",
    "    #print('TensorList[0] =',  TensorList[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss function approaches = tensor(0.1092, grad_fn=<NormBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(LostList)\n",
    "print('loss function approaches =', LostList[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
